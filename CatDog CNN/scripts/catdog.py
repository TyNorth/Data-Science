# -*- coding: utf-8 -*-
"""CatDog Complete.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EyGyoL_A4G12sUxRVAy4MruGobig3o3Z
"""

#import libraries

import os, shutil
import tensorflow as tf
from tensorflow.keras.layers import Flatten, Dense, MaxPooling2D, Conv2D, Dropout 
from tensorflow.keras.optimizers import RMSprop 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import models
import numpy as np

#create subfolders for training, testing, and vakidation sets
#using small set to mimick real world setting where tens of thousands of data points is not always possible

original_dataset_dir = '~/Documents/Documents/train/'
base_dir = 'C:/Users/tnort/Documents/cats_and_dogs_small/'
#os.mkdir(base_dir)

train_dir = os.path.join(base_dir, 'train')
#os.mkdir(train_dir)

validation_dir = os.path.join(base_dir, 'validation')
#os.mkdir(validation_dir)

test_dir = os.path.join(base_dir, 'test')
#os.mkdir(test_dir)

train_cats_dir = os.path.join(train_dir, 'cats')
#os.mkdir(train_cats_dir)

train_dogs_dir = os.path.join(train_dir, 'dogs')
#os.mkdir(train_dogs_dir)

validation_cats_dir = os.path.join(validation_dir, 'cats')
#os.mkdir(validation_cats_dir)

validation_dogs_dir = os.path.join(validation_dir, 'dogs')
#os.mkdir(validation_dogs_dir)

test_cats_dir = os.path.join(test_dir, 'cats')
#os.mkdir(test_cats_dir)

test_dogs_dir = os.path.join(test_dir, 'dogs')
#os.mkdir(test_dogs_dir)

#copies first 1000 images to train, 500 to test, 500 to validation for dogs and cats

fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]
for fname in fnames:
  src = os.path.join(original_dataset_dir, fname)
  dst = os.path.join(train_cats_dir, fname)
  shutil.copyfile(src, dst)

fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]
for fname in fnames:
  src = os.path.join(original_dataset_dir, fname)
  dst = os.path.join(validation_cats_dir, fname)
  shutil.copyfile(src, dst)

fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]
for fname in fnames:
  src = os.path.join(original_dataset_dir, fname)
  dst = os.path.join(test_cats_dir, fname)
  shutil.copyfile(src, dst)

fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]
for fname in fnames:
  src = os.path.join(original_dataset_dir, fname)
  dst = os.path.join(train_dogs_dir, fname)
  shutil.copyfile(src, dst)

fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]
for fname in fnames:
  src = os.path.join(original_dataset_dir, fname)
  dst = os.path.join(validation_dogs_dir, fname)
  shutil.copyfile(src, dst)

fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]
for fname in fnames:
  src = os.path.join(original_dataset_dir, fname)
  dst = os.path.join(test_dogs_dir, fname)
  shutil.copyfile(src, dst)

#Number of images per training, testing, and validation set

print('total training cat images:', len(os.listdir(train_cats_dir)), '\n',
      'total training dog images:', len(os.listdir(train_dogs_dir)), '\n',
      'total validation cat images:', len(os.listdir(validation_cats_dir)), '\n',
      'total validation dog images:', len(os.listdir(validation_dogs_dir)), '\n',
      'total test cat images:', len(os.listdir(test_cats_dir)), '\n',
      'total test dog images:', len(os.listdir(test_dogs_dir)), '\n'
      )

#model blueprint, summary statistics, 3.4 million parameters

model_blueprint = Sequential()

model_blueprint.add(Conv2D(32,
                kernel_size = (3, 3),
                activation = 'relu',
                input_shape = (150, 150, 3)))
model_blueprint.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))

model_blueprint.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))
model_blueprint.add(MaxPooling2D(pool_size = (2,2)))

model_blueprint.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))
model_blueprint.add(MaxPooling2D(pool_size = (2,2)))

model_blueprint.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))
model_blueprint.add(MaxPooling2D(pool_size = (2,2)))

model_blueprint.add(Flatten())
model_blueprint.add(Dense(512, activation = 'relu'))
model_blueprint.add(Dense(1, activation = 'sigmoid'))

model_blueprint.summary()

#defining convolutional neural network  

RMS = RMSprop(lr=1e-4)

def convolutional_model(model_name = str):
  model_name = Sequential()



  model_name.add(Conv2D(32,
                  kernel_size = (3, 3),
                  activation = 'relu',
                  input_shape = (150, 150, 3)))
  model_name.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))

  model_name.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))
  model_name.add(MaxPooling2D(pool_size = (2,2)))

  model_name.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))
  model_name.add(MaxPooling2D(pool_size = (2,2)))

  model_name.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))
  model_name.add(MaxPooling2D(pool_size = (2,2)))

  model_name.add(Flatten())
  model_name.add(Dense(512, activation = 'relu'))
  model_name.add(Dense(1, activation = 'sigmoid'))



  model_name.compile(optimizer = RMS, loss = 'binary_crossentropy', metrics = ['accuracy'])
  return model_name

#rescales all images by 1/255

def create_train_datagen():
  train_datagen = ImageDataGenerator(rescale = 1./255)
  return train_datagen

train_datagen = create_train_datagen()

def create_test_datagen():
  test_datagen = ImageDataGenerator(rescale = 1./255)
  return test_datagen

test_datagen = create_test_datagen()


#resizes all triaining images to 150 x 150
def create_train_generator(batch_size = int, target_size = (int, int), class_mode =  'binary' ):
  train_generator = train_datagen.flow_from_directory(
      train_dir,
      target_size = (150, 150),
      batch_size = 20,
      class_mode = 'binary')
  return train_generator

train_generator = create_train_generator(batch_size = 20, target_size = (150,150))

#resizes all validation images to 150 x 150
def create_validation_generator(batch_size = int, target_size = (int, int), class_mode = 'binary'):
  validation_generator = test_datagen.flow_from_directory(
      validation_dir,
      target_size = (150, 150),
      batch_size = 20,
      class_mode = 'binary')
  return validation_generator

validation_generator = create_validation_generator(20, (150,150))

#checking the shape of batches
for data_batch, labels_batch in train_generator:
  print('data batch shape : ', data_batch.shape, '\n',
        'labels batch shape : ', labels_batch.shape)
  break

#Model fit function
def history(model = tf.keras.models.Sequential, steps_per_epoch = int, epochs = int, validation_steps = int): #takes model and three int parameters
  history = model.fit(train_generator,
                      steps_per_epoch=steps_per_epoch,
                      epochs=epochs,
                      validation_data=validation_generator,
                      validation_steps=validation_steps)
  return history

#fitting/training the neural network

model_one = convolutional_model('baseline_model')

history1 = history(model_one, 100, 30, 50)

#save model for later use
model_one.save('cat_dog_mini_one1.h5')

#Defining accuracy and loss graphs

import matplotlib.pyplot as plt


def acc_vloss(history_model = str):
  acc = history_model.history['accuracy']
  val_acc = history_model.history['val_accuracy']
  loss = history_model.history['loss']
  val_loss = history_model.history['val_loss']
  epochs = range(1, len(acc) + 1)
  plt.plot(epochs, acc, 'bo', label='Training acc')
  plt.plot(epochs, val_acc, 'b', label='Validation acc')
  plt.title('Training and validation accuracy')
  plt.legend()
  plt.figure()
  plt.plot(epochs, loss, 'bo', label='Training loss')
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.legend()
  plt.show()
  
acc_vloss(history1)

#model is overfitting badly
#setting up data augmentation to mitigate overfitting on small dataset

datagen = ImageDataGenerator(rotation_range=40,
                           width_shift_range=0.2,
                           height_shift_range=0.2,
                           shear_range=0.2,
                           zoom_range=0.2,
                           horizontal_flip=True,
                           fill_mode='nearest')

#new convnet with dropout layer to mitigate overfitting

def convolutional_dropout_model(model_name =str):
  model_name = Sequential()

  model_name.add(Conv2D(32,
                  kernel_size = (3, 3),
                  activation = 'relu',
                  input_shape = (150, 150, 3)))
  model_name.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))

  model_name.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))
  model_name.add(MaxPooling2D(pool_size = (2,2)))

  model_name.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))
  model_name.add(MaxPooling2D(pool_size = (2,2)))

  model_name.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))
  model_name.add(MaxPooling2D(pool_size = (2,2)))

  model_name.add(Flatten())
  model_name.add(Dropout(0.5))
  model_name.add(Dense(512, activation = 'relu'))
  model_name.add(Dense(1, activation = 'sigmoid'))

  model_name.compile(optimizer = RMS, loss = 'binary_crossentropy', metrics = ['accuracy'])
  return model_name

#Generating augmented images

def create_train_datagen_aug():
  train_datagen_aug = ImageDataGenerator(rescale=1./255,
                                    rotation_range=40,
                                    width_shift_range=0.2,
                                    height_shift_range=0.2,
                                    shear_range=0.2,
                                    zoom_range=0.2,
                                    horizontal_flip=True,)
  return train_datagen_aug

train_datagen_aug = create_train_datagen_aug()

test_gen =create_test_datagen()

train_generator = create_train_generator(32, (150, 150))

validation_generator = create_validation_generator((150,150), 32)

#training/fitting neural network with dropout layer

model_two = convolutional_dropout_model('dropout_model')
history2 = history(model_two, 100, 100, 50)

#saving second model
model_two.save('cat_dog_mini_two2.h5')

#plot accuracy and loss
acc_vloss(history2)

#Using a pretrained model

from tensorflow.keras.applications import VGG16

#instantiating VGG16 convolutional base with 14.7 mil parameters

convolutional_base = VGG16(weights = 'imagenet',
                            include_top = False,
                            input_shape = (150, 150, 3))

convolutional_base.summary()

"""Adding convolutionaal base model to neural network without data augmentation for demo purposes"""

#extracting features with pretrained model
#recording the convolutional base output to np.array

#datagen = ImageDataGenerator(rescale=1./255)
#batch_size = 20

#def extract_features(directory, sample_count):
  #features = np.zeros(shape=(sample_count, 4, 4, 512))
  #labels = np.zeros(shape=(sample_count))
  #generator = datagen.flow_from_directory(
  #directory,
  #target_size=(150, 150),
  #batch_size=batch_size,
  #class_mode='binary')

  #i = 0

  #for inputs_batch, labels_batch in generator:
    #features_batch = convolutional_base.predict(inputs_batch) #calling predict extracts features
    #features[i * batch_size : (i + 1) * batch_size] = features_batch
    #labels[i * batch_size : (i + 1) * batch_size] = labels_batch
    #i += 1

    #if i * batch_size >= sample_count:
      #break

  #return features, labels

#train_features, train_labels = extract_features(train_dir, 2000)

#validation_features, validation_labels = extract_features(validation_dir, 1000)

#test_features, test_labels = extract_features(test_dir, 1000)

#Defining new model eith Dense layers to compliment VGG16 convolutional base

#RMS = RMSprop(lr = 2e-5)
#def VGG16_base_dense():
  #model = Sequential()

  #model.add(Dense(256, activation = 'relu', input_dim = 4 * 4 * 512))
  #model.add(Dropout(0.5))
  #model.add(Dense(1, activation = 'sigmoid'))

  #model.compile(optimizer = RMS, loss = 'binary_crossentropy', metrics = ['accuracy'])
  #return model

#training third model with VGG16 convolutional base, no data augmentation

#model_three = VGG16_base_dense()

#history3 = history = model_three.fit(train_features, train_labels,
#                               epochs = 30,
#                               batch_size =20,
#                               validation_data =(validation_features, validation_labels))

#acc_vloss()

#model_three.save('cat_dog_mini_three.h5')

"""#Using a pretrained model with data augmentation"""

#This method is slower and more expensive but allows data augmentation during training, which is great for smaller datasets
#Adding convolutional base and Dense layers

def convolutional_base_dense_model(model_name = str):
  model_name = Sequential()

  model_name.add(convolutional_base)
  model_name.add(Flatten())
  model_name.add(Dense(256, activation = 'relu'))
  model_name.add(Dense(1, activation = 'sigmoid'))
  return model_name

#Freezing convolutional_base trainable parameters  
convolutional_base.trainable = False

train_datagen_aug = create_train_datagen_aug()

test_datagen = create_test_datagen()

train_generator = create_train_generator(20, (150, 150), 'binary')

validation_generator = create_validation_generator(20, (150, 150), 'binary')

model_four = convolutional_base_dense_model('trained_plus')

model_four.compile(optimizer = RMS,
              loss = 'binary_crossentropy',
              metrics = ['accuracy'])

history4 = history(model_four, 100, 30, 50)

model_four.save('cat_dog_mini_four4.h5')

acc_vloss(history4)

"""#Fine-Tuning"""

#unfreezing some layers
convolutional_base.summary()

#unfreezing block5_pool & block5_conv1 - 3
convolutional_base.trainable = True

set_trainable = False 

for layer in convolutional_base.layers:
  if layer.name == 'block5_conv1':
    set_trainable = True
  if set_trainable:
    layer.trainable = True
  else:
    layer.trainable = False

len(convolutional_base.trainable_weights)

RMS = RMSprop(lr=1e-5)
model_four.compile(loss = 'binary_crossentropy', optimizer = RMS, metrics = ['accuracy'] )

history5 = history(model_four, 100, 100, 50)

acc_vloss(history5)

#evaluating the final model
def create_test_generator(batch_size = int, target_size = (int, int), class_mode = 'binary'):
  test_generator = test_datagen.flow_from_directory(
  test_dir,
  target_size=(150, 150),
  batch_size=20,
  class_mode='binary')
  return test_generator
  
test_loss, test_accuracy = model_four.evaluate(create_test_generator(20, (150, 150), 'binary'), steps = 50)
print('test acc:', test_accuracy)

model_four.save('cat_dog_cnn_final_model5.h5')

model_four.summary()

